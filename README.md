### Observabilidade ETL - Pipeline Completo

<p align="center">
  <a href="https://suajornadadedados.com.br/"><img src="https://github.com/lvgalvao/data-engineering-roadmap/raw/main/pics/logo.png" alt="Jornada de Dados"></a>
</p>
<p align="center">
    <em>Nossa miss√£o √© fornecer o melhor ensino em engenharia de dados</em>
</p>

Bem-vindo a **Jornada de Dados**

# Projeto de Observabilidade: ETL Completa para Convers√£o de CSV em Parquet

Este reposit√≥rio cont√©m um projeto desenvolvido para implementar e monitorar uma Pipeline ETL completa que converte arquivos CSV para o formato Parquet. Focado em boas pr√°ticas de engenharia de dados, o objetivo √© mostrar como observar e monitorar cada etapa da pipeline em tempo real utilizando ferramentas como logs, m√©tricas e traces.

## Este projeto faz parte da Jornada de Dados

Assista ao v√≠deo completo aqui

[![Imagem](./pics/workshop_thumbnail.png)](https://youtube.com/live/z1EOlFV8g7g)

## Arquitetura

```mermaid
flowchart TD
    User[Usu√°rio] -->|Configura e executa a ETL| ETL[Pipeline ETL]
    ETL -->|Logs e M√©tricas| Observabilidade
    Observabilidade -->|Alertas em tempo real| Dashboard[Plataforma de Monitoramento]
    ETL --> Storage[Armazenamento Parquet]
    Storage -->|Consulta e an√°lise| BI[Plataforma de BI]
```

---

## O que voc√™ vai aprender neste projeto?

1. **Cria√ß√£o de uma ETL do Zero**: Como configurar e implementar uma Pipeline ETL funcional.
2. **Logs e Observabilidade**: Monitoramento detalhado para rastrear cada etapa da transforma√ß√£o de dados.
3. **M√©tricas e Traces**: Como medir desempenho e identificar gargalos em pipelines complexas.
4. **Armazenamento em Parquet**: Por que este formato √© eficiente para data lakes.
5. **Integra√ß√£o com Ferramentas**: Como integrar logs com ferramentas como ELK Stack, Prometheus e Grafana.

---

## Pr√©-requisitos

1. **Python 3.8+**
2. **Bibliotecas necess√°rias** (dispon√≠veis no arquivo `requirements.txt`):
   - `pandas`
   - `pyarrow`
   - `requests`
   - `logging`
   - `opentelemetry`
   - `prometheus_client`
   - `flask`
3. **Ferramentas de Monitoramento**:
   - **Grafana**: Para criar dashboards e monitorar m√©tricas.
   - **ElasticSearch + Kibana**: Para gerenciar logs.

Para instalar as depend√™ncias:
```bash
pip install -r requirements.txt
```

---

## Estrutura do Projeto

### M√≥dulos Principais

#### 1. **`etl.py`**
Este m√≥dulo executa as etapas principais da ETL:
- Leitura de arquivos CSV.
- Transforma√ß√£o dos dados.
- Escrita no formato Parquet.

#### 2. **`logger.py`**
- Implementa logs detalhados para rastrear erros e passos cr√≠ticos.
- Integra√ß√£o com ElasticSearch para an√°lise centralizada.

#### 3. **`metrics.py`**
- Define m√©tricas para monitorar desempenho (tempo de execu√ß√£o, registros processados, etc.).
- Integra√ß√£o com Prometheus.

#### 4. **`tracer.py`**
- Configura traces para identificar gargalos na pipeline.
- Usa OpenTelemetry para instrumenta√ß√£o.

---

## Configura√ß√£o

### 1. Configura√ß√£o do `.env`
Crie um arquivo `.env` na raiz do projeto para configurar par√¢metros necess√°rios:
```env
LOG_SERVER=http://localhost:9200
PROMETHEUS_URL=http://localhost:9090
DATA_DIR=./data
OUTPUT_DIR=./output
```

### 2. Estrutura de Pastas
```bash
‚îú‚îÄ‚îÄ data/                # Diret√≥rio com arquivos CSV de entrada
‚îú‚îÄ‚îÄ output/              # Diret√≥rio para arquivos Parquet gerados
‚îú‚îÄ‚îÄ logs/                # Diret√≥rio para logs locais
‚îú‚îÄ‚îÄ etl.py               # Script principal da pipeline ETL
‚îú‚îÄ‚îÄ logger.py            # M√≥dulo para logs detalhados
‚îú‚îÄ‚îÄ metrics.py           # M√≥dulo para monitoramento de m√©tricas
‚îú‚îÄ‚îÄ tracer.py            # M√≥dulo para traces e gargalos
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias do projeto
```

---

## Como Executar?

### 1. Configurar o Ambiente
- Instale as depend√™ncias com `pip install -r requirements.txt`.
- Configure o arquivo `.env`.

### 2. Executar o Script
Para rodar a ETL:
```bash
python etl.py
```

### 3. Visualizar Logs e M√©tricas
- **Grafana**: Acesse os dashboards de m√©tricas em `http://localhost:3000`.
- **ElasticSearch**: Veja os logs detalhados no Kibana.

---

## Exemplos de Monitoramento

1. **Logs**: 
   - Logs detalhados de cada etapa da pipeline.
   - Exemplo:
     ```plaintext
     INFO - Iniciando leitura do arquivo CSV: data/input.csv
     INFO - Transforma√ß√£o conclu√≠da: 10.000 registros processados.
     INFO - Arquivo Parquet salvo em: output/data.parquet
     ```

2. **M√©tricas**:
   - Total de registros processados.
   - Tempo de execu√ß√£o por etapa.
   - Exemplo:
     ```plaintext
     etl_records_processed_total{status="success"} 10000
     etl_execution_time_seconds{step="transform"} 2.5
     ```

3. **Traces**:
   - Identifica√ß√£o de gargalos.
   - Exemplo: Lat√™ncia alta durante a escrita no Parquet.

---

## Extens√µes Futuras

- **Alertas via Telegram**: Configurar notifica√ß√µes autom√°ticas em caso de falhas na pipeline.
- **Integra√ß√£o com Data Lake**: Adicionar suporte a armazenamento em S3 ou Azure Blob.
- **Dashboards Avan√ßados**: Criar visualiza√ß√µes customizadas no Grafana.

---

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Siga as etapas abaixo para colaborar:
1. Fa√ßa um fork do reposit√≥rio.
2. Crie uma branch para suas altera√ß√µes:
   ```bash
   git checkout -b feature/nova-funcionalidade
   ```
3. Envie um pull request.

---

### **üöÄ Comece sua Jornada de Dados com Observabilidade!**